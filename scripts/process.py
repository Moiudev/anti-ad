import hashlib
import idna
import json
import os
import re
import requests
import shutil
from concurrent.futures import ThreadPoolExecutor, as_completed
from converter import convert_to_json
from pathlib import Path


# åŸºç¡€è·¯å¾„é…ç½®
BASE_DIR = Path(__file__).parent.parent
SOURCES_DIR = BASE_DIR / "rules"
SOURCES_LIST = BASE_DIR / "sources.txt"
OUTPUT_JSON = BASE_DIR / "ad.json"
HASH_CACHE = BASE_DIR / "hash_cache.json"

# è¯·æ±‚å¤´é…ç½®
REQUEST_HEADERS = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/125.0.0.0 Safari/537.36",
    "Accept": "text/plain, */*",
    "Accept-Language": "en-US,en;q=0.9",
    "Connection": "keep-alive",
    "Cache-Control": "no-cache",
    "Pragma": "no-cache",
}

# è§„åˆ™æºçš„åç§°æ˜ å°„
FRIENDLY_NAME_MAP = {
    "https://easylist-downloads.adblockplus.org/easylist.txt": "EasyList.txt",
    "https://easylist-downloads.adblockplus.org/easylistchina.txt": "EasyList_China.txt",
    "https://easylist-downloads.adblockplus.org/easyprivacy.txt": "EasyPrivacy.txt",
    "https://hblock.molinero.dev/hosts_adblock.txt": "HBlock_AdBlock.txt",
    "https://phishing.army/download/phishing_army_blocklist.txt": "Phishing_Army.txt",
    "https://raw.githubusercontent.com/AdguardTeam/FiltersRegistry/master/filters/filter_2_Base/filter.txt": "AdGuard_Base.txt",
    "https://raw.githubusercontent.com/AdguardTeam/FiltersRegistry/master/filters/filter_3_Spyware/filter.txt": "AdGuard_Spyware.txt",
    "https://raw.githubusercontent.com/AdguardTeam/FiltersRegistry/master/filters/filter_11_Mobile/filter.txt": "AdGuard_Mobile.txt",
    "https://raw.githubusercontent.com/AdguardTeam/FiltersRegistry/master/filters/filter_14_Annoyances/filter.txt": "AdGuard_Annoyances.txt",
    "https://raw.githubusercontent.com/AdguardTeam/FiltersRegistry/master/filters/filter_15_DnsFilter/filter.txt": "AdGuard_DNS.txt",
    "https://raw.githubusercontent.com/AdguardTeam/FiltersRegistry/master/filters/filter_224_Chinese/filter.txt": "AdGuard_Chinese.txt",
    "https://raw.githubusercontent.com/badmojr/1Hosts/master/Pro/adblock.txt": "1Hosts_ADBlock_Pro.txt",
    "https://raw.githubusercontent.com/cjx82630/cjxlist/master/cjx-annoyance.txt": "CJX_Annoyance.txt",
    "https://raw.githubusercontent.com/damengzhu/banad/main/dnslist.txt": "BanAD_DNS.txt",
    "https://raw.githubusercontent.com/damengzhu/banad/main/jiekouAD.txt": "BanAD_JiekouAD.txt",
    "https://raw.githubusercontent.com/FiltersHeroes/KADhosts/master/KADhosts.txt": "KADhosts.txt",
    "https://raw.githubusercontent.com/hagezi/dns-blocklists/main/adblock/pro.txt": "Hagezi_ADBlock_Pro.txt",
    "https://raw.githubusercontent.com/Loyalsoldier/v2ray-rules-dat/release/reject-list.txt": "Loyalsoldier_Reject_List.txt",
    "https://raw.githubusercontent.com/StevenBlack/hosts/master/alternates/fakenews-gambling/hosts": "StevenBlack_Fakenews_Gambling.txt",
    "https://raw.githubusercontent.com/TG-Twilight/AWAvenue-Ads-Rule/main/AWAvenue-Ads-Rule.txt": "AWAvenue_Ads_Rule.txt",
    "https://raw.githubusercontent.com/uBlockOrigin/uAssets/master/filters/filters.txt": "uBlock_Filters.txt",
    "https://raw.githubusercontent.com/xinggsf/Adblock-Plus-Rule/master/mv.txt": "ChengFeng_MV.txt",
    "https://raw.githubusercontent.com/xinggsf/Adblock-Plus-Rule/master/rule.txt": "ChengFeng_Rule.txt",
    "https://someonewhocares.org/hosts": "DanPollock_Hosts.txt",
}


# åŠ è½½è§„åˆ™æºåˆ—è¡¨
def load_sources():
    with open(SOURCES_LIST, "r", encoding="utf-8") as f:
        return [line.strip() for line in f if line.strip() and not line.startswith("#")]


# ç”Ÿæˆå®‰å…¨çš„æ–‡ä»¶å
def safe_filename(url):
    if url in FRIENDLY_NAME_MAP:
        return FRIENDLY_NAME_MAP[url]

    # å¯¹äºŽä¸åœ¨æ˜ å°„è¡¨ä¸­çš„ URLï¼Œä½¿ç”¨åŽŸå§‹æ–‡ä»¶å
    filename = re.sub(r"[^\w\.-]", "_", url.split("/")[-1].split("?")[0])
    return filename or "unnamed.txt"


# éªŒè¯åŸŸåæ˜¯å¦æœ‰æ•ˆ (æ”¯æŒ IDN åŸŸå)
def is_valid_domain(domain):
    # æ£€æŸ¥åŸºæœ¬é•¿åº¦é™åˆ¶
    if len(domain) > 253 or len(domain) < 1:
        return False

    # æ£€æŸ¥é¦–å°¾å­—ç¬¦
    if domain.startswith(".") or domain.endswith("."):
        return False

    # å°è¯•å¤„ç†å›½é™…åŒ–åŸŸå (IDN)
    try:
        domain = idna.encode(domain).decode("ascii")
    except idna.IDNAError:
        return False

    # éªŒè¯åŸŸåç»“æž„
    pattern = r"^([a-z0-9]|[a-z0-9][a-z0-9\-]{0,61}[a-z0-9])(\.([a-z0-9]|[a-z0-9][a-z0-9\-]{0,61}[a-z0-9]))*$"
    return re.match(pattern, domain) is not None


# æ ‡å‡†åŒ–åŸŸå
def normalize_domain(domain):
    domain = domain.lower()
    if domain.startswith("www."):
        return domain[4:]
    return domain


# ä¸‹è½½å•ä¸ªè§„åˆ™æº
def download_single(url, cache):
    try:
        filename = safe_filename(url)
        filepath = SOURCES_DIR / filename

        print(f"ä¸‹è½½ä¸­: {url}")
        response = requests.get(url, headers=REQUEST_HEADERS, timeout=25)
        response.raise_for_status()

        # è®¡ç®—å†…å®¹å“ˆå¸Œ
        content = response.text
        content_hash = hashlib.md5(content.encode("utf-8")).hexdigest()

        # æ£€æŸ¥æ˜¯å¦éœ€è¦æ›´æ–°
        if url in cache and cache[url] == content_hash and filepath.exists():
            print(f"è·³è¿‡æœªæ›´æ–°: {filename}")
            return url, None

        # ä¿å­˜æ–‡ä»¶
        with open(filepath, "w", encoding="utf-8") as f:
            f.write(content)

        print(f"ä¸‹è½½æˆåŠŸ: {filename}")
        return url, content_hash
    except requests.exceptions.Timeout:
        print(f"ä¸‹è½½è¶…æ—¶ [{url}]")
        return url, None
    except Exception as e:
        print(f"ä¸‹è½½å¤±è´¥ [{url}]: {str(e)}")
        return url, None


# ä¸‹è½½æ‰€æœ‰è§„åˆ™æº (æ”¯æŒå¢žé‡æ›´æ–°)
def download_rules():
    # ç¡®ä¿ç›®å½•å­˜åœ¨
    SOURCES_DIR.mkdir(parents=True, exist_ok=True)

    # åŠ è½½å“ˆå¸Œç¼“å­˜
    try:
        with open(HASH_CACHE, "r", encoding="utf-8") as f:
            cache = json.load(f)
    except:
        cache = {}

    rule_sources = load_sources()
    total = len(rule_sources)

    print(f"å¼€å§‹å¤„ç† {total} ä¸ªè§„åˆ™æº...")

    # ä½¿ç”¨çº¿ç¨‹æ± å¹¶è¡Œä¸‹è½½
    updated_cache = cache.copy()
    with ThreadPoolExecutor(max_workers=5) as executor:
        futures = [executor.submit(download_single, url, cache) for url in rule_sources]

        for future in as_completed(futures):
            url, new_hash = future.result()
            if new_hash:
                updated_cache[url] = new_hash

    # ä¿å­˜æ›´æ–°åŽçš„ç¼“å­˜
    with open(HASH_CACHE, "w", encoding="utf-8") as f:
        json.dump(updated_cache, f, indent=2)

    print("æ‰€æœ‰è§„åˆ™æºå¤„ç†å®Œæˆï¼")


# å°†è§„åˆ™åˆ†ç±»ä¸ºå®Œæ•´åŸŸåã€åŸŸååŽç¼€æˆ–æ­£åˆ™è¡¨è¾¾å¼
def classify_rule(rule_str):
    rule = rule_str.strip()

    # è·³è¿‡æ³¨é‡Šå’Œä¾‹å¤–è§„åˆ™
    if not rule or rule.startswith(("!", "#", "@@", "//", "[")) or "##" in rule:
        return None

    # å¤„ç† ||domain^ æ ¼å¼ - è½¬æ¢ä¸ºåŸŸååŽç¼€åŒ¹é…
    if rule.startswith("||") and rule.endswith("^"):
        domain = rule[2:-1]
        if domain and is_valid_domain(domain):
            # æ ‡å‡†åŒ–åŸŸåå¹¶ä½œä¸ºåŽç¼€è§„åˆ™
            return ("suffix", normalize_domain(domain))

    # å¤„ç†å®Œæ•´åŸŸåè§„åˆ™ - è½¬æ¢ä¸ºåŽç¼€è§„åˆ™
    if is_valid_domain(rule):
        return ("suffix", normalize_domain(rule))

    # å¤„ç†hostsæ ¼å¼è§„åˆ™
    if re.match(r"^\d{1,3}\.\d{1,3}\.\d{1,3}\.\d{1,3}\s+", rule):
        domains = re.split(r"\s+", rule)[1:]
        return [
            ("suffix", normalize_domain(d))
            for d in domains
            if d and not d.startswith(("#", "!")) and is_valid_domain(d)
        ]

    # å¤„ç†æ­£åˆ™è¡¨è¾¾å¼è§„åˆ™
    if rule.startswith("/") and rule.endswith("/"):
        regex_pattern = rule[1:-1]
        # ç®€å•éªŒè¯æ­£åˆ™è¡¨è¾¾å¼æœ‰æ•ˆæ€§
        try:
            re.compile(regex_pattern)
            return ("regex", regex_pattern)
        except re.error:
            return None

    # å¤„ç†åŸŸååŽç¼€è§„åˆ™ (*.example.com)
    if rule.startswith("*.") and is_valid_domain(rule[2:]):
        return ("suffix", normalize_domain(rule[2:]))

    # å¤„ç†é€šé…ç¬¦åŸŸåè§„åˆ™
    if "*" in rule and "." in rule and not any(c in rule for c in ["/", ":", "!", "#"]):
        # å°è¯•è½¬æ¢ä¸ºåŸŸååŽç¼€
        if rule.startswith("*.") and is_valid_domain(rule[2:]):
            return ("suffix", normalize_domain(rule[2:]))

        # å°è¯•è½¬æ¢ä¸ºæ­£åˆ™è¡¨è¾¾å¼
        try:
            # å°†é€šé…ç¬¦è½¬æ¢ä¸ºæ­£åˆ™è¡¨è¾¾å¼
            regex_pattern = rule.replace(".", r"\.").replace("*", ".*")
            re.compile(regex_pattern)  # éªŒè¯æœ‰æ•ˆæ€§
            return ("regex", regex_pattern)
        except re.error:
            return None

    return None


# å¤„ç†æ‰€æœ‰è§„åˆ™æ–‡ä»¶
def process_rules():
    suffix_rules = set()
    regex_rules = set()

    # èŽ·å–æ‰€æœ‰è§„åˆ™æ–‡ä»¶
    rule_files = list(SOURCES_DIR.glob("*"))
    total_files = len(rule_files)

    if total_files == 0:
        print("è­¦å‘Š: æ²¡æœ‰æ‰¾åˆ°ä»»ä½•è§„åˆ™æ–‡ä»¶ï¼")
        return {"suffix": [], "regex": []}

    print(f"å¼€å§‹å¤„ç† {total_files} ä¸ªè§„åˆ™æ–‡ä»¶...")

    for i, file in enumerate(rule_files, 1):
        try:
            print(f"å¤„ç†æ–‡ä»¶ ({i}/{total_files}): {file.name}")
            with open(file, "r", encoding="utf-8", errors="ignore") as f:
                for line in f:
                    result = classify_rule(line)
                    if not result:
                        continue

                    # å¤„ç†å•æ¡è§„åˆ™æˆ–å¤šæ¡è§„åˆ™
                    if isinstance(result, list):
                        for item in result:
                            rule_type, value = item
                            if rule_type == "suffix":
                                suffix_rules.add(value)
                            elif rule_type == "regex":
                                regex_rules.add(value)
                    else:
                        rule_type, value = result
                        if rule_type == "suffix":
                            suffix_rules.add(value)
                        elif rule_type == "regex":
                            regex_rules.add(value)
        except Exception as e:
            print(f"å¤„ç†æ–‡ä»¶ {file.name} æ—¶å‡ºé”™: {str(e)}")

    # è¿”å›žå¤„ç†åŽçš„è§„åˆ™
    return {"suffix": sorted(suffix_rules), "regex": sorted(regex_rules)}


def main():
    print("----- å¼€å§‹è§„åˆ™å¤„ç† -----")
    download_rules()

    print("----- å¤„ç†è§„åˆ™å†…å®¹ -----")
    rules_dict = process_rules()

    print("----- ç”Ÿæˆè§„åˆ™é›† -----")
    convert_to_json(rules_dict, OUTPUT_JSON)

    # æ‰“å°ç»Ÿè®¡ä¿¡æ¯
    suffix_count = len(rules_dict["suffix"])
    regex_count = len(rules_dict["regex"])
    total = suffix_count + regex_count
    print(
        f"è§„åˆ™ç»Ÿè®¡: åŸŸååŽç¼€ - {suffix_count}, æ­£åˆ™è¡¨è¾¾å¼ - {regex_count}, æ€»è®¡ - {total}"
    )
    print("å¤„ç†å®Œæˆï¼ðŸŽ‰ðŸŽ‰ðŸŽ‰")


if __name__ == "__main__":
    main()
